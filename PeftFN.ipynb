{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0N9hbYvS13_"
      },
      "outputs": [],
      "source": [
        "## Install required libraries\n",
        "!pip install -q transformers datasets accelerate bitsandbytes peft trl torch\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q llama-cpp-python\n",
        "!pip install -q ctranslate2\n",
        "!pip install --upgrade transformers accelerate peft trl torch\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhdEL2mkS2EV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFX1u4KdS2Mq"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"google/gemma-2-2b-it\"  # Replace with the actual model name.\n",
        "\"\"\"Supported models:\n",
        "______________________________________________\n",
        "|                                            |\n",
        "|  1.google/gemma-2-9b                       |\n",
        "|  2.google/gemma-2-2b                       |\n",
        "|  3.google/gemma-2-2b-it                    |\n",
        "|  4.google/codegemma-2b                     |\n",
        "|  5.google/codegemma-7b-it                  |\n",
        "|  6.microsoft/Phi-3-mini-4k-instruct        |\n",
        "|  7.microsoft/Phi-3-mini-128k-instruct      |\n",
        "|  8.microsoft/Phi-3-medium-128k-instruct    |\n",
        "|  9.Qwen/Qwen2-7B-Instruct                  |   \n",
        "|  10.Qwen/Qwen2-7B-Instruct-AWQ             |\n",
        "|  11.meta-llama/Meta-Llama-3.1-8B           |  \n",
        "|____________________________________________|  \n",
        "And many more\n",
        "\"\"\"\n",
        "\n",
        "# use_auth_token=\"hf-token_from_huggingface\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\n",
        "                                        #   use_auth_token=\"\"\n",
        "                                        )\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "## Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    # use_auth_token=\"\"\n",
        ")\n",
        "\n",
        "## Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "## LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "## Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Loading cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhf0hEPqQc7B"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "        Processes example data to generate formatted text using a given prompt and end-of-sequence token.\n",
        "\n",
        "        This function extracts instructions, inputs, and outputs from the provided examples dictionary.\n",
        "        It then formats these elements using the given alpaca_prompt and appends the EOS_TOKEN to each formatted text.\n",
        "        Finally, it returns a dictionary containing the list of formatted texts.\n",
        "\n",
        "        Args:\n",
        "            examples (dict): A dictionary containing example data with keys such as \"instruction\", \"input\", \"output\", etc.\n",
        "            alpaca_prompt (str): A string format to be used for generating the formatted text.\n",
        "            EOS_TOKEN (str): A token to be appended at the end of each formatted text.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary with a single key \"text\" containing a list of formatted texts.\n",
        "    \"\"\"\n",
        "    \n",
        "    instructions = examples.get(\"instruction\", examples.get(\"instruction\",examples.get(\"system\", [])))\n",
        "    inputs = examples.get(\"input\", examples.get(\"input\",examples.get(\"command\", [])))\n",
        "    outputs = examples.get(\"output\", examples.get(\"Output\", examples.get(\"response\", examples.get(\"Response\", []))))\n",
        "\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        \n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts} # Return a dictionary as expected by map function\n",
        "\n",
        "def standardize_columns(dataset):\n",
        "    \"\"\"\n",
        "        Standardizes the column names of a given dataset.\n",
        "\n",
        "        This function renames specific columns in the dataset to standardized names.\n",
        "        The renaming is based on a predefined dictionary that maps old column names to new ones.\n",
        "        Only the columns present in the dataset will be renamed.\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): The dataset whose columns need to be standardized. \n",
        "                            It should have a method `rename_columns` that accepts a dictionary for renaming.\n",
        "\n",
        "        Returns:\n",
        "            Dataset: The dataset with standardized column names.\n",
        "    \"\"\"\n",
        "    rename_dict = {\n",
        "        \"Response\": \"output\",\n",
        "        \"response\": \"output\",\n",
        "        \"Output\": \"output\",\n",
        "        \"Input\": \"input\",\n",
        "        \"Instruction\": \"instruction\",\n",
        "        \"Instruction\": \"system\"\n",
        "    }\n",
        "    return dataset.rename_columns({k: v for k, v in rename_dict.items() if k in dataset.column_names})\n",
        "\n",
        "\n",
        "# Insert the Dataset repo_id below to load the dataset from the repo: \n",
        "\n",
        "datasets_to_load = [\n",
        "  \"ICEPVP8977/Debian_Hacking_Networking\",\n",
        "]\n",
        "\n",
        "def has_train_split(dataset_name):\n",
        "    \"\"\"\n",
        "        Check if a dataset has a 'train' split.\n",
        "\n",
        "        This function attempts to load a dataset and checks if it contains a 'train' split.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): The name of the dataset to check.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the dataset has a 'train' split, False otherwise or if an error occurs.\n",
        "\n",
        "        Raises:\n",
        "            None: Exceptions are caught and False is returned.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataset_info = load_dataset(dataset_name, split=None)\n",
        "        return 'train' in dataset_info.keys()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "datasets_with_train_split = [dataset_name for dataset_name in datasets_to_load if has_train_split(dataset_name)]\n",
        "\n",
        "datasets = []\n",
        "for dataset_name in datasets_with_train_split:\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, split=\"train\")\n",
        "        standardized_dataset = standardize_columns(dataset)\n",
        "\n",
        "        required_columns = [\"instruction\", \"input\", \"output\"]\n",
        "        if all(col in standardized_dataset.column_names for col in required_columns):\n",
        "            datasets.append(standardized_dataset)\n",
        "            print(f\"Successfully loaded and standardized: {dataset_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping {dataset_name}: Missing required columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
        "\n",
        "combined_dataset = concatenate_datasets(datasets)\n",
        "\n",
        "formatted_dataset = combined_dataset.map(formatting_prompts_func, batched=True, remove_columns=combined_dataset.column_names)\n",
        "\n",
        "formatted_dataset = formatted_dataset.shuffle(seed=199)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training configurations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEW7J0cUTDXP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define training arguments without max_seq_length\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,#Default = 4\n",
        "    per_device_train_batch_size=2,# default is 4, for low vram set to 1 or 2 depending on your GPU.\n",
        "    gradient_accumulation_steps=2,# default is 4, for low vram set to 1 or 2 depending on your GPU.\n",
        "    warmup_steps=100,\n",
        "    learning_rate=2e-4,# For general and fast adaptation {5e-5} is generally recommended./-- For the model to reproduce the exact text from the datasets the learning rate {1e-5} or even lower.\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFX_YJ6xZK0k"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use a data collator for dynamic padding\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Set to False for causal language modeling\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv0lqBEgZLC8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,  # Use the data collator\n",
        "    packing=False,\n",
        "    max_seq_length=8192  # Set the maximum sequence length here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3lxm7EZTG6l"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save to 4 bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFwfqVoEkfwu"
      },
      "outputs": [],
      "source": [
        "model = trainer.model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Gdpd-Bkg4A"
      },
      "outputs": [],
      "source": [
        "# Save the full model in 4 bit\n",
        "model.save_pretrained(\"./full_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzhpMaPkjLG"
      },
      "outputs": [],
      "source": [
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"./full_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save to f16/32 \n",
        "### I recommend considering the f16/32 format for its ease of conversion to the GGUF format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = model.to(torch.float32)\n",
        "# Now convert the model to FP16\n",
        "model_fp16_path = \"./model_fp16\"\n",
        "model.half()  # Convert model to FP16, comment this line out to keep the f32 float model\n",
        "model.save_pretrained(model_fp16_path)\n",
        "tokenizer.save_pretrained(model_fp16_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# After training is complete\n",
        "\n",
        "# Save the LoRA adapter\n",
        "# trainer.model.save_pretrained(\"./lora_adapter\")\n",
        "MODEL_NAME=\"google/gemma-2-2b-it\" \n",
        "# Load the original base model in FP16\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    # use_auth_token=\"\"\n",
        ")\n",
        "\n",
        "# Load the LoRA adapter\n",
        "adapter_model = PeftModel.from_pretrained(base_model, \"./model_fp16\")\n",
        "\n",
        "# Merge the LoRA weights with the base model\n",
        "merged_model = adapter_model.merge_and_unload()\n",
        "\n",
        "# Save the full merged model in FP16\n",
        "merged_model.save_pretrained(\"./full_model_fp16\", safe_serialization=True)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"./full_model_fp16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gc\n",
        "\n",
        "# Free VRAM\n",
        "del base_model\n",
        "del adapter_model\n",
        "del merged_model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the merged model and tokenizer\n",
        "merged_model = AutoModelForCausalLM.from_pretrained(\"./full_model_fp16\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./full_model_fp16\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"How do I capture the handshake for WPA/WPA2 networks using Aircrack-ng?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ANSWER:\n",
        "To capture the WPA/WPA2 handshake using Aircrack-ng, you need to follow these steps:\n",
        "\n",
        "1. **Put the Wireless Interface in Monitor Mode:**\n",
        "   You need to switch your wireless interface to monitor mode to capture packets. This can be done using the `airmon-ng` tool.\n",
        "\n",
        "   ```bash\n",
        "   airmon-ng start wlan0\n",
        "   ```\n",
        "\n",
        "   Replace `wlan0` with the name of your wireless interface.\n",
        "\n",
        "2. **Start Airodump-ng to Capture the Handshake:**\n",
        "   Use `airodump-ng` to capture the 4-way WPA/WPA2 handshake. You need to specify the channel and the BSSID (MAC address) of the access point you are targeting.\n",
        "\n",
        "   ```bash\n",
        "   airodump-ng -c 9 --bssid 00:14:6C:7E:40:80 -w psk ath0\n",
        "   ```\n",
        "\n",
        "   Here:\n",
        "   - `-c 9` specifies the channel of the wireless network.\n",
        "   - `--bssid 00:14:6C:7E:40:80` specifies the MAC address of the access point.\n",
        "   - `-w psk` sets the file name prefix for the capture file.\n",
        "   - `ath0` is the interface name in monitor mode.\n",
        "\n",
        "3. **Optional: Deauthenticate a Client to Force Reconnection:**\n",
        "   If you want to speed up the process, you can use `aireplay-ng` to deauthenticate a client connected to the network, forcing it to reconnect and thus capturing the handshake.\n",
        "\n",
        "   ```bash\n",
        "   aireplay-ng --deauth 50 -a 00:14:6C:7E:40:80 -c 00:0F:B5:FD:FB:C2 ath0\n",
        "   ```\n",
        "\n",
        "   Here:\n",
        "   - `--deauth 50` sends 50 deauthentication packets.\n",
        "   - `-a 00:14:6C:7E:40:80` specifies the MAC address of the access point.\n",
        "   - `-c 00:0F:B5:FD:FB:C2` specifies the MAC address of the client to deauthenticate.\n",
        "\n",
        "4. **Verify the Handshake Capture:**\n",
        "   Once `airodump-ng` captures the handshake, you will see a message indicating that a WPA handshake has been captured.\n",
        "\n",
        "   ```bash\n",
        "   WPA handshake: 00:14:6C:7E:40:80\n",
        "   ```\n",
        "\n",
        "5. **Crack the Handshake:**\n",
        "   After capturing the handshake, you can use `aircrack-ng` to crack the WPA/WPA2 pre-shared key using a dictionary attack.\n",
        "\n",
        "   ```bash\n",
        "   aircrack-ng -w wordlist.txt -b 00:14:6C:7E:40:80 psk-01.cap\n",
        "   ```\n",
        "\n",
        "   Here:\n",
        "   - `-w wordlist.txt` specifies the wordlist file.\n",
        "   - `-b 00:14:6C:7E:40:80` specifies the BSSID of the access point.\n",
        "   - `psk-01.cap` is the capture file containing the handshake.\n",
        "\n",
        "By following these steps, you can capture and attempt to crack the WPA/WPA2 pre-shared key using Aircrack-ng."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt = \"Your_question_here\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(merged_model.device)\n",
        "max_new_tokens = 2000  # Set the maximum number of tokens in the response\n",
        "outputs = merged_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert to GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!pip install -r ./llama.cpp/requirements.txt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p /content/gguf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!/content/llama.cpp/convert_hf_to_gguf.py /content/full_model_fp16 --outfile /content/gguf_model/output_file.gguf --outtype q8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push to huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login,HfApi\n",
        "\n",
        "\n",
        "login(token=\"\")\n",
        "api = HfApi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Uploads the \"full\" trained model to the Hugging Face Hub\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/full_model_fp16\",\n",
        "    repo_id=\"\",#\"your_repo_id\",\n",
        "    repo_type=\"model\"  # specify the type of repository\n",
        ")\n",
        "# Uploads the \"gguf\" trained model to the Hugging Face Hub\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/gguf_model\",\n",
        "    repo_id=\"\",#\"your_repo_id\",\n",
        "    repo_type=\"model\"  # specify the type of repository\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
